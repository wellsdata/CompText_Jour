---
title: "KEY oct 19 bigrams"
author: "Rob Wells"
date: '2024-10-15'
output: html_document
---

# Jour 389/689 Fall 2024:

Prep for Assignment #3: Basic pipeline proficiency

Use this dataset of Los Angeles Times articles on Chinese business in the U.S.

ChinaFDI-LAT_tidy.csvLinks to an external site.

Load the appropriate software libraries

Import the data

Then tokenize the data, one word per row

Generate a list of the top 20 bigrams

Create a ggplot chart showing the top 20 bigrams

Create a table showing the top five bigrams per year

At the bottom of the R markdown document, write a 250 word memo describing your key findings, focusing on how the bigrams changed over time. Describe any problems you encountered in this process.

 

Edit Rubric together

```{r message=FALSE, warning=FALSE}
#load tidyverse, tidytext, rio 
library(tidyverse)
library(rio)
library(tidytext)
```

```{r}
#Import dataframe 

fdi <- read_csv("https://raw.githubusercontent.com/wellsdata/CompText_Jour/main/data/ChinaFDI-LAT_tidy.csv")

```




# Count the number of distinct articles in 1910
```{r}
fdi %>% 
  select(article_nmbr) %>% 
 distinct(article_nmbr, .keep_all = TRUE) %>% 
  count(article_nmbr) %>% 
  summarize(total =sum(n)) 
#There are 36 distinct articles in the dataset 
n_distinct(fdi$article_nmbr)
```

#Remove stopwords
```{r}
data(stop_words)
```


# Bigrams

```{r}
bigrams <- fdi %>% 
  select(text) %>% 
  mutate(text = str_squish(text)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "times|accountid|docview|searchproquestcom|proquest|los|angeles|calif|document|id|url|searchcom|title|pages|publication date|publication subject|issn|language of publication: english|document url|copyright|last updated|database|startofarticle|proquest document id|classification|https|--|people|publication info|illustration|caption|[0-9.]|identifier /keyword|twitter\\.", "")) |> 
  mutate(text = str_replace_all(text, "- ", "")) %>% 
  unnest_tokens(bigram, text, token="ngrams", n=2 ) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

bigrams
```

#top 20 bigrams
```{r}

top_20_bigrams <- bigrams |> 
   top_n(20) |> 
  mutate(bigram = paste(word1, " ", word2)) |> 
  select(bigram, n)
  


```



```{r}
ggplot(top_20_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
  geom_bar(stat = "identity") +
  theme(legend.position = "none") +
  coord_flip() +  
  labs(title = "Top Two-Word phrases about Chinese investment in U.S.",
       caption = "n=36 articles. Graphic by Rob Wells. 10-21-2024",
       x = "Phrase",
       y = "Count of terms")
```



```{r}
afinn <- get_sentiments("afinn")

#Clean and tokenize
sentiment <- fdi %>% 
  select(text, article_nmbr) %>% 
  mutate(text = str_squish(text)) |> 
  mutate(text = tolower(text)) |>  
  mutate(text = str_replace_all(text, "times|accountid|docview|searchproquestcom|proquest|los|angeles|calif|document|id|url|searchcom|title|pages|publication date|publication subject|issn|language of publication: english|document url|copyright|last updated|database|startofarticle|proquest document id|classification|https|--|people|publication info|illustration|caption|[0-9.]|identifier /keyword|twitter\\.", "")) |> 
  mutate(text = str_replace_all(text, "- ", "")) |> 
  group_by(article_nmbr) |> 
  unnest_tokens(tokens, text) |> 
  filter(!tokens %in% stop_words$word) 



# Sentiment analysis by joining the tokenized words with the AFINN lexicon
sentiment_analysis <- sentiment %>%
  inner_join(afinn, by = c("tokens"="word")) %>%
  group_by(article_nmbr) %>%  
  summarize(sentiment = sum(value), .groups = "drop")

# a column for sentiment type (Positive or Negative)
sentiment_analysis <- sentiment_analysis %>%
   group_by(article_nmbr) %>% 
  mutate(sentiment_type = ifelse(sentiment >= 0, "Positive", "Negative"))
```

```{r}
# Visualize the sentiment scores
ggplot(sentiment_analysis, aes(x = article_nmbr, y = sentiment, fill = sentiment_type)) +
  geom_col(position = "dodge", stat = "identity") +
  scale_fill_manual(values = c("Positive" = "darkgreen", "Negative" = "red")) + 
  labs(title = "Sentiment Analysis Using AFINN Lexicon",
       caption = "n=36 articles. Graphic by Rob Wells 10-20-2024",
       x = "Articles",
       y = "Sentiment Score") 
```


