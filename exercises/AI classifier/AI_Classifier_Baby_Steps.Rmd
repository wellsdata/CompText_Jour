---
title: "AI Classifier"
output: html_notebook
date: 04-18-2025
---

This exercise is an introduction to using AI to classify content. We're using the ellmer package: <https://ellmer.tidyverse.org/>

Background on ellmer

ellmer supports a wide variety of model providers:

Anthropicâ€™s Claude: chat_claude(). AWS Bedrock: chat_bedrock(). Azure OpenAI: chat_azure(). Databricks: chat_databricks(). DeepSeek: chat_deepseek(). GitHub model marketplace: chat_github(). Google Gemini: chat_gemini(). Groq: chat_groq(). Ollama: chat_ollama(). OpenAI: chat_openai(). OpenRouter: chat_openrouter(). perplexity.ai: chat_perplexity(). Snowflake Cortex: chat_snowflake() and chat_cortex_analyst(). VLLM: chat_vllm()

chat_gemini() is great for large prompts because it has a much larger context window than other models. It allows up to 1 million tokens. It also comes with a generous free tier (with the downside that your data is used to improve the model).

And so we will use chat_gemini()

```{r}
#install.packages("ellmer")
library(ellmer)
library(tidyverse)
library(glue)
library(janitor)

```

```{r}
chat <- chat_gemini()

#Prompt
chat$chat("Please give me three reasons to get out of bed in the morning. Make it funny")
```

## Load Data

library(googledrive)
library(rio)

# First, download the file from Google Drive
temp_file <- tempfile()
drive_download(as_id("1fexSMTg_wJOpyh_XCuc_3_2CP_FGS_tB"), path = temp_file)

# Then import the downloaded file
# If you know the file format, specify it explicitly
dei_articles <- rio::import(temp_file, format = "csv") # or "xlsx", "txt", etc.

```{r echo = F}
#136596 rows of text
articles_text <-  rio::import("https://www.dropbox.com/scl/fi/0xej3ebt6kuiwxa0czpq4/moley_cleaned_perspective_text.csv?rlkey=wo7akc0jl99uxnzrru9hrrh5a&st=4wclfmx3&dl=0&raw=1") |> 
    mutate(year = as.numeric(year),
         date = as.Date(pubdate, format="%b %d, %Y"))
```

# Load a very small subset: 10 articles

```{r}

llm_test <- articles_text[1:1000, ]

llm_test <- llm_test |> 
  mutate(sentence = gsub("\n", " ", sentence)) |> 
  mutate(sentence = gsub("\\s+", " ", sentence))

```

# Industrial strength cleaning and processing

--Articles put into a single string of text, no punctuation or spaces

```{r}
process_articles <- function(df) {
  # First group by article identifiers and concatenate sentences
  # Using filename as the identifier, but you might need to use other columns
  compiled_df <- df %>%
    group_by(filename) %>%
    summarize(
      # Combine all non-NA sentences
      sentence = paste(na.omit(sentence), collapse = " "),
      # Keep one value for each of the other columns
      title = first(title),
      abstract = first(abstract),
      store_id = first(store_id),
      article_type = first(article_type),
      authors = first(authors),
      copyright = first(copyright),
      document_type = first(document_type),
      entry_date = first(entry_date),
      issn = first(issn),
      issue = first(issue),
      language = first(language),
      language_of_summary = first(language_of_summary),
      pages = first(pages),
      place_of_publication = first(place_of_publication),
      pubdate = first(pubdate),
      pubtitle = first(pubtitle),
      year = first(year),
      volume = first(volume),
      document_url = first(document_url),
      document_features = first(document_features),
      start_page = first(start_page),
      find_a_copy = first(find_a_copy),
      database = first(database),
      date = first(date)
    )
  
  # Remove punctuation and spaces from the sentence column
  compiled_df <- compiled_df %>%
    mutate(sentence = str_replace_all(sentence, "[[:punct:][:space:]]", "")) |> 
    mutate(sentence = tolower(sentence))
  
  return(compiled_df)
}

# Apply the function to your dataframe
processed_llm_test <- process_articles(llm_test)
```

# The AI prompt

```{r}
# Prepare data with filenames and content
articles_for_analysis <- processed_llm_test %>%
  select(filename, sentence) %>%
  mutate(article_data = paste("FILENAME:", filename, "\nCONTENT:", sentence))

# Combine with clear separators
combined_text_with_filenames <- paste(articles_for_analysis$article_data, 
                                    collapse = "\n\n---ARTICLE SEPARATOR---\n\n")

# Update system prompt to request the specific format you want
chat <- chat_gemini(
  system_prompt = "You are an academic researcher performing context analysis on selected Newsweek articles. Analyze each article and classify it based on these criteria:
  
  1. If an article contains two or more mentions of President Richard Nixon, classify it as 'nixon'
  2. If an article contains two or more mentions of vote or voter registration, classify it as 'voter_turnout'
  3. If an article contains two or more mentions the Vietnam War, classify it as 'vietnam'
  4. If an article has adjectives that criticize Democrats, classify it as 'democrat_critique'
  5. If an article meets multiple criteria, assign multiple categories separated by semicolons
  6. If an article meets none of the criteria, classify it as 'other'
  
  Return ONLY a CSV-formatted result with exactly two columns:
  filename,category
  
  For example:
  filename,category
  article1.txt,nixon
  article2.txt,voter_turnout
  article3.txt,nixon;voter_turnout
  article4.txt,other
  
  No additional text, explanations, or summary counts."
)

# Send to the LLM
response <- chat$chat(combined_text_with_filenames)
```

#old version lacks the explanation column

```{r}
process_llm_response_to_df <- function(response) {
  # Extract lines
  lines <- strsplit(response, "\n")[[1]]
  
  # Remove markdown code block markers if present
  lines <- lines[!grepl("^```", lines)]
  
  # Initialize vectors for data
  filenames <- c()
  categories <- c()
  
  # Flag to track if we're processing data (after header)
  header_found <- FALSE
  
  for (line in lines) {
    # Skip empty lines
    if (trimws(line) == "") next
    
    # Skip row numbers or other artifacts (lines with asterisks)
    if (grepl("\\*\\*", line)) next
    
    # Check if this is the header line
    if (grepl("filename,category", line, ignore.case = TRUE)) {
      header_found <- TRUE
      next
    }
    
    # Process data lines (only after header is found)
    if (header_found) {
      # Split by comma, handling potential issues
      parts <- strsplit(line, ",")[[1]]
      if (length(parts) >= 2) {
        filenames <- c(filenames, parts[1])
        # Handle case where category might contain commas
        categories <- c(categories, paste(parts[2:length(parts)], collapse=","))
      }
    }
  }
  
  # Create and return the dataframe
  if (length(filenames) > 0) {
    data.frame(
      filename = filenames,
      category = categories,
      stringsAsFactors = FALSE
    )
  } else {
    # Return empty dataframe with correct structure if no data found
    data.frame(
      filename = character(0),
      category = character(0),
      stringsAsFactors = FALSE
    )
  }
}

result_df2 <- process_llm_response_to_df(response2)

result_df2

```

# validate results

democrat critique

```{r}
dem <- result_df2 |> 
  filter(str_detect(category, "democrat_critique")) |> 
  mutate(path = paste0("./perspective_extracted/",filename)) |> 
  mutate(ai_correct = " ",
         ai_wrong = " ",
         unsure = " ",
         notes = " ")

write.csv(dem, "dem_ai_verification.csv")


file_paths1 <- dem$path
dem_list <- lapply(file_paths1, readLines)
```

# process the results to a single file, separated by file name

```{r}

write_combined_files <- function(file_list, file_paths, output_file) {
  # Open connection to output file
  con <- file(output_file, "w")
  
  # Loop through each file
  for (i in seq_along(file_list)) {
    # Extract just the filename from the path
    filename <- basename(file_paths[i])
    
    # Write the separator with filename
    writeLines(paste0("=== FILE: ", filename, " ==="), con)
    
    # Write the content of the file
    writeLines(file_list[[i]], con)
    
    # Add a blank line between files (except after the last file)
    if (i < length(file_list)) {
      writeLines("", con)
    }
  }
  
  # Close the connection
  close(con)
  
  # Return a message
  message(paste("Successfully wrote", length(file_list), "files to", output_file))
}

write_combined_files(viet_list, file_paths1, "combined_viet_files.txt")
```

Now, open a Google Sheet, import dem_ai_verification.csv, read the articles and rate the responses

# A better AI prompt

```{r}
chat2 <- chat_gemini(
  system_prompt = "You are an academic researcher performing context analysis on selected Newsweek articles. Analyze each article and classify it based on these criteria:
  
  1. If an article contains two or more mentions of President Richard Nixon, classify it as 'nixon'
  2. If an article contains two or more mentions of vote or voter registration, classify it as 'voter_turnout'
  3. If an article contains two or more mentions the Vietnam War, classify it as 'vietnam'
  4. If an article contains two or more mentions of Democrats or the Democratic Party and if adjectives modifying Democrats are critical, classify it as 'democrat_critique'
  5. If an article meets multiple criteria, assign multiple categories separated by semicolons
  6. If an article meets none of the criteria, classify it as 'other'
  
  Return ONLY a CSV-formatted result with exactly THREE (3) columns:
  filename, category, explanation
  
  For example:
  filename, category, explanation
  article1.txt,nixon, nixon mentioned twice
  article2.txt,democrat_critique, these words were critical of democrats: xyz
  article3.txt,nixon;voter_turnout, nixon mentioned three times voter turnout twice
  article4.txt,other
  
  No additional text, explanations, or summary counts."
)

# Send to the LLM
response2 <- chat2$chat(combined_text_with_filenames)
```

```{r}
process_llm_response_to_df <- function(response) {
  # Extract lines
  lines <- strsplit(response, "\n")[[1]]
  
  # Remove markdown code block markers if present
  lines <- lines[!grepl("^```", lines)]
  
  # Initialize vectors for data
  filenames <- c()
  categories <- c()
  explanations <- c()
  
  # Flag to track if we're processing data (after header)
  header_found <- FALSE
  
  for (line in lines) {
    # Skip empty lines
    if (trimws(line) == "") next
    
    # Skip row numbers or other artifacts (lines with asterisks)
    if (grepl("\\*\\*", line)) next
    
    # Check if this is the header line
    if (grepl("filename.*category.*explanation", line, ignore.case = TRUE)) {
      header_found <- TRUE
      next
    }
    
    # Process data lines (only after header is found)
    if (header_found || grep("batch.*\\.txt", line)) {
      # Parse each row with better handling of commas within explanations
      parts <- strsplit(line, ",\\s*", perl = TRUE, fixed = FALSE)[[1]]
      
      if (length(parts) >= 1) {
        filename <- parts[1]
        category <- ifelse(length(parts) >= 2, parts[2], "")
        
        # Handle the explanation (everything after second comma)
        if (length(parts) >= 3) {
          explanation <- paste(parts[3:length(parts)], collapse = ", ")
        } else {
          explanation <- NA
        }
        
        filenames <- c(filenames, filename)
        categories <- c(categories, category)
        explanations <- c(explanations, explanation)
      }
    }
  }
  
  # Create and return the dataframe
  if (length(filenames) > 0) {
    data.frame(
      filename = filenames,
      category = categories,
      explanation = explanations,
      stringsAsFactors = FALSE
    )
  } else {
    # Return empty dataframe with correct structure if no data found
    data.frame(
      filename = character(0),
      category = character(0),
      explanation = character(0),
      stringsAsFactors = FALSE
    )
  }
}
result_df2 <- process_llm_response_to_df(response2)
```

#vietnam

```{r}
viet <- result_df2 |> 
  filter(str_detect(category, "vietnam")) |> 
  mutate(path = paste0("./perspective_extracted/",filename)) |> 
  mutate(ai_correct = " ",
         ai_wrong = " ",
         unsure = " ",
         notes = " ")

write.csv(viet, "viet_ai_verification.csv")


file_paths1 <- viet$path
viet_list <- lapply(file_paths1, readLines)
```

# process the results to a single file, separated by file name

```{r}

write_combined_files <- function(file_list, file_paths, output_file) {
  # Open connection to output file
  con <- file(output_file, "w")
  
  # Loop through each file
  for (i in seq_along(file_list)) {
    # Extract just the filename from the path
    filename <- basename(file_paths[i])
    
    # Write the separator with filename
    writeLines(paste0("=== FILE: ", filename, " ==="), con)
    
    # Write the content of the file
    writeLines(file_list[[i]], con)
    
    # Add a blank line between files (except after the last file)
    if (i < length(file_list)) {
      writeLines("", con)
    }
  }
  
  # Close the connection
  close(con)
  
  # Return a message
  message(paste("Successfully wrote", length(file_list), "files to", output_file))
}

write_combined_files(viet_list, file_paths1, "combined_viet_files.txt")
```

```{r}
# chat <- chat_ollama(
#   model = "deepseek-r1:70b"
# )

```

```{r}

chat <- chat_gemini()

data <- chat$extract_data(
  "My name is Marcus and I'm a 24 year old jazz player",
  type = type_object(
    age = type_number(),
    name = type_string(),
    major = type_string()
  )
)

as.data.frame(data)

```

The amount you trust an LLM is related to the amount of pain you're willing to endure.

time management - limiot to five queries per minute to stay in free tier. put a five second pause at the return after the function call

library(refiner) Open Refine tool key_collision_merge <https://cran.r-project.org/web/packages/refineR/vignettes/refineR_package.html> <https://r-packages.io/packages/refineR/refineR-Package>

```{r}


chat <- chat_gemini()

chat$extract_data(
  "POSS W/ INTENT DIST METH",
  type = type_object(
    methamphetamine_related = type_boolean(),
    drug_possession_related = type_boolean(),
    drug_distribution_related = type_boolean(),
    fully_spelled_out_no_abbreviations = type_string()
  )
)
```

```{r}

countypopchange <- read_csv("https://the-art-of-data-journalism.github.io/tutorial-data/census-estimates/nebraska.csv")

statenarrative <- countypopchange |> 
  select(COUNTY, STATE, CTYNAME, STNAME, POPESTIMATE2023, POPESTIMATE2022, NPOPCHG2023, NATURALCHG2023, NETMIG2023) |>
  mutate(POPPERCENTCHANGE = ((POPESTIMATE2023-POPESTIMATE2022)/POPESTIMATE2022)*100) |> 
  mutate(GEOID = paste0(COUNTY, STATE)) |> 
  arrange(desc(POPPERCENTCHANGE)) |> 
  mutate(PCTCHANGERANK = row_number()) |> 
  mutate(base_narrative = glue(
  "County: {CTYNAME}, Population in 2023: {POPESTIMATE2023}, Population in 2022: {POPESTIMATE2022}, Population change: {NPOPCHG2023}, Percent change: {POPPERCENTCHANGE}, Percent change rank in {STNAME}: {PCTCHANGERANK}, Natural change (births vs deaths): {NATURALCHG2023}, Net migration: {NETMIG2023}")) 

statenarrative$base_narrative[[1]]

```

```{r}

chat <- chat_gemini(
  system_prompt = "You are a demographics journalist from Nebraska. Your job today is to write short -- 2-3 sentence -- summaries of population estimates from the Census Bureau for each county in the state. I will provide you the name of the county and a series of population numbers for the county. Your job is to turn it into a concise but approachable summary of what happened in that county. Here is the data you have to work with: "
)

chat$chat(statenarrative$base_narrative[[2]])
```
