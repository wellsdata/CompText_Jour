---
title: "Bigrams Exercise Sept 24"
author: "Rob Wells"
date: '2024-09-20'
output: html_document
---

# Jour 389/689 Fall 2024:

Use the following code as a template with a new dataset:

black_press_extracted_text_june_22_2024.csv

See the blue hashtag instructions in the code chunks, such as:

#import df

#Show range of years covered

#Create chart of years

And do the same with the new dataset: black_press_extracted_text_june_22_2024.csv

Hand in your completed assignment by uploading to your personal GitHub repository, obtaining the URL and submitting that to Elms

```{r message=FALSE, warning=FALSE}
#load tidyverse, tidytext, rio and quanteda libraries
library(tidyverse)
library(rio)
library(tidytext)
library(quanteda)
library(knitr)


```

```{r}
#Import dataframe 

lynch <- read_csv("../data/articles_oct_19.csv")

#wells only
#lynch <- read_csv("/Users/robwells/Code/CompText_Jour/data/articles_oct_19.csv")
```


# Create a new dataframe that filters articles for 1900 to 1910

```{r}

lynch1910 <-  lynch %>% 
  filter(year >= 1900 & year <= 1910)
```


# Count the number of distinct articles in 1910
```{r}
lynch1910 %>% 
  select(filename) %>% 
 distinct(filename, .keep_all = TRUE) %>% 
  count(filename) %>% 
  summarize(total =sum(n)) 
#There are 1,732 distinct articles in the dataset for the 1900-1910 
#or the Marilyn Harbert method!:
n_distinct(lynch1910$filename)
```

# Count the number of newspaper_states in the 1900 corpus
```{r}

states1900 <- lynch1910 %>% 
  select(newspaper_state, filename) %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  count(newspaper_state) %>% 
  arrange(desc(n))

#and now provide code to list the top five states
states1900 %>% 
  select(newspaper_state, n) %>% 
slice_max(n, n=10)

 # 1 Wisconsin          91
 # 2 Arkansas           90
 # 3 Mississippi        89
 # 4 Nebraska           83
 # 5 Utah               80

```

# Tokenize the 1900 lynching stories

```{r}

stories <- str_replace_all(lynch1910$sentence, "- ", "")
stories_df <- tibble(stories,)

# unnest includes lower, punct removal

stories_tokenized <- stories_df %>%
  unnest_tokens(word,stories)

stories_tokenized
```


#Remove stopwords
The tidytext package includes the stop_words dataset. We will load it below. It contains, as of this writing, 1,149 words that data scientist and linguistic nerds felt could be removed from sentences because they don't add meaning. Filtering out these words can help focus on the more meaningful content, making it easier to uncover trends, themes, and key information in large amounts of text. Obviously, we have different priorities and we may or may not want to use stop_words or we have want to provide a customized list.

The list is derived from three separate lists, or lexicons: SMART (571 words), onix (404 words), and snowball (174 words)

The ONIX lexicon comes from the Open Information Exchange and is often used in text mining and natural language processing. 

The Snowball lexicon is part of a broader project that has algorithms that simplify words in different languages by reducing them to their root form. It's best known for the Porter stemming algorithm, which, for example, changes "running" to "run." 

Lastly, the SMART lexicon is a set of common words, like "and," "the," and "is," and it comes from the SMART Information Retrieval System, created at Cornell University in the 1960s.

```{r}
data(stop_words)

test <- stop_words %>% 
  as.data.frame()

head(test)
```

```{r}

stories_tokenized <- stories_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))

# Word Count

story_word_ct <- stories_tokenized %>%
  count(word, sort=TRUE)

head(story_word_ct)

#write_csv(lynch_word_ct, "lynching_corpus_word_count.csv")

```

# Bigrams
## We are now creating two word phrases but before the stop words are taken out

```{r}
stories_bigrams <- stories_df %>%
  unnest_tokens(bigram, stories, token="ngrams", n=2)

stories_bigrams_separated <- stories_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

stories_bigram_cts <- stories_bigrams_separated %>%
  count(word1, word2, sort = TRUE)
stories_bigram_cts
```

## Now the counts with the filter
```{r}

stories_bigrams_filtered <- stories_bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

stories_bigram_cts2 <- stories_bigrams_filtered %>%
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

stories_bigram_cts2
```


```{r}

#replace Date for the decade analyzed
stories_bigram_cts_1900 <- stories_bigram_cts %>% 
  mutate(decade = "1900s")

#write_csv(stories_bigram_cts_post1940, "../output/post1940_lynch_bigram_count.csv")

```


# And here is a MUCH tighter version of the process above:
```{r}
stories_bigrams_cts3 <- lynch1910 %>% 
  select(sentence) %>% 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1)) %>% 
   mutate(decade = "1900s")

  stories_bigrams_cts3 
```

# YOUR TURN

Create one dataframe with black press articles
```{r}
bp <- lynch %>% 
  filter(black_press=="Y")


```


#BP bigrams
```{r}
bp_stories_bigrams <- stories_df %>%
  unnest_tokens(bigram, stories, token="ngrams", n=2)

bp_stories_bigrams_separated <- bp_stories_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

bp_stories_bigrams_separated

```


Create one dataframe without black press articles
```{r}
wp <- lynch %>% 
  filter(is.na(black_press))

#for some reason, the column had a trailing space so the anti-filter only worked like so:
# wp <- lynch %>% 
#   filter(black_press != "Y ")

#to eliminate leading and trailing spaces
# lynch <- lynch %>%
#   mutate(black_press = trimws(black_press)) 


```


Produce the top 20 bigrams for the black press and non-black press coverage
# Tokenize the 1900 lynching stories

```{r}

wp_stories_bigrams <- wp %>% 
  select(sentence) %>% 
  mutate(sentence = str_replace_all(sentence, "- ", "")) %>% 
  unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

wp_stories_bigrams 
```

Compare and discuss!
#top BP Bigrams
```{r}
bp_top5 <- head(bp_stories_bigrams, 5)

cat("The top five Black press bigrams were:\n")
print(bp_top5)

```

#top WP Bigrams
```{r}
wp_top5 <- head(wp_stories_bigrams, 5)

cat("The top five white press bigrams were:\n")
print(wp_top5)


```

print(paste0("A: The top five cities in the newspaper_city column are: ", 
             paste(top_five, collapse = ", "), ". However, it's important to note that the dataset of ",nrow(blackindex_master), " rows has ", newspaper_city_count_na, " rows with a missing value in the newspaper_city column."))


```{r}

# Create a data frame with the top bigrams
results_table <- data.frame(
  Rank = 1:5,
  `Black Press Bigrams` = bp_top5,
  `White Press Bigrams` = wp_top5
)

#fix the column names
results_table <- results_table %>% 
  rename(bp1 = 2, bp2=3, bp_count=4, wp1=5, wp6=6, wp_count=7)

results_table
```


```{r}
library(kableExtra)

# Formatting so you can read it in R studio Dark Mode
kable(results_table, 
      caption = "Top Five Bigrams in Black and White Press",
      align = c('c', 'l', 'l')) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                full_width = FALSE,
                position = "left",
                font_size = 14) %>%
  row_spec(0, bold = TRUE, color = "white", background = "#4C4C4C") %>%
  column_spec(1, bold = TRUE, border_right = TRUE) %>%
  row_spec(1:5, background = "white", color = "black")
```


# Trigrams

```{r}
stories_trigrams <- stories_df %>%
  unnest_tokens(trigram, stories, token="ngrams", n=3)

stories_trigrams_separated <- stories_trigrams %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

stories_trigrams_ct <- stories_trigrams_separated %>%
  count(word1, word2, word3, sort = TRUE)

#filtered
# stories_trigrams_filtered <- stories_trigrams_separated %>%
#   filter(!word1 %in% stop_words$word) %>%
#   filter(!word2 %in% stop_words$word) %>%
#   filter(!word3 %in% stop_words$word)
# 
# stories_trigrams_ct <- stories_trigrams_filtered %>%
#   count(word1, word2, word3, sort = TRUE)

#replace Date for the decade analyzed
stories_trigrams_ct_post1940 <- stories_trigrams_ct %>% 
  mutate(decade = "post1940")

write_csv(stories_trigrams_ct_post1940, "../output/post1940_lynch_trigram_count.csv")


```

#Compile DFs

```{r}
stories_bigram_cts_pre1850s <- read.csv("../output/pre1850s_lynch_bigram_count.csv")
stories_bigram_cts_the1850s <- read.csv("../output/the1850s_lynch_bigram_count.csv")
stories_bigram_cts_the1860s <- read.csv("../output/1860s_lynch_bigram_count.csv")
stories_bigram_cts_the1870s <- read.csv("../output/1870s_lynch_bigram_count.csv")
stories_bigram_cts_the1880s <- read.csv("../output/1880s_lynch_bigram_count.csv")
stories_bigram_cts_the1890s <- read.csv("../output/1890s_lynch_bigram_count.csv")
stories_bigram_cts_the1900s <- read.csv("../output/1900s_lynch_bigram_count.csv")
stories_bigram_cts_the1910  <- read.csv("../output/1910s_lynch_bigram_count.csv")
stories_bigram_cts_the1920s <- read.csv("../output/1920s_lynch_bigram_count.csv")
```

```{r}
#Compile DFs

bigrams_all <- rbind(stories_bigram_cts_pre1850s,stories_bigram_cts_the1850s, stories_bigram_cts_the1860s, stories_bigram_cts_the1870s, stories_bigram_cts_the1880s, stories_bigram_cts_the1890s, stories_bigram_cts_the1900s, stories_bigram_cts_the1910,stories_bigram_cts_the1920s, stories_bigram_cts_1930s, stories_bigram_cts_post1940) 


write.csv(bigrams_all, "../output/all_bigrams_11.10.csv")
```

```{r}
stories_QUINTgrams <- stories_df %>%
  unnest_tokens(phrase, stories, token="ngrams", n=5)

stories_QUINTgrams_ct <- stories_QUINTgrams %>%
  count(phrase, sort=TRUE)

#write_csv(stories_QUINTgrams_ct, "stories_corpus_quintgram_count.csv")

stories_QUINTgrams_ct

```

```{r}
# plotting for fun and profit
#NEEDS TO BE FIXED
story_word_ct %>%
  filter(n >= 5000) %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(n, word)) + 
  geom_col() +
  labs(y = NULL) + 
  ggtitle("Words mentioned 5000 times or more in identified lynching articles")

```

*If we want to look at trigrams for narrative, maybe we don't drop the stop words? Might be more likely to catch turns of phrase.*

# Quanteda

```{r}
#install.packages("readtext")
library(quanteda)
library(readtext)
lynch1 <- readtext("../narratives/article_text_test")
```

### creates index with metadata

```{r}
###
# List out text files that match pattern .txt, create DF
###

files <- list.files("../article_text_7_15/article_text/", pattern="*.txt") %>% 
  as.data.frame() %>%
  rename(filename = 1) %>%
  filter(!str_detect(filename,"log"))


###
# Load 638 stories provided by jack, create join column, join to files list
###

jackindex <- read_csv("../article_text_7_15/article_text/LayoutBoxes_index.csv") %>%
  mutate(filename = paste0(file_id,"_",article_id,".txt")) %>%
  inner_join(files) %>%
  mutate(filepath = paste0("../article_text_7_15/article_text/",filename))
```

### adds metadata to corpus

```{r}
lynch1 <- lynch1 %>% 
  inner_join(jackindex, by=c("doc_id"="filename"))


#Other options
#summary(corpus_subset(data_corpus_inaugural, President == "Adams"))

```

```{r}

my_corpus <- corpus(lynch1)  # build a new corpus from the texts
summary(my_corpus)


```

### subset corpus

```{r}

x1920s <- summary(corpus_subset(my_corpus, year > 1920))

```

## kwic

```{r}

quanteda_test <- kwic(my_corpus, "lynch", valuetype = "regex") %>% as.data.frame()


quanteda_test <- kwic(my_corpus, "torture", valuetype = "regex") %>% as.data.frame()

quanteda_test <- kwic(my_corpus, "watts", valuetype = "regex") %>% as.data.frame()

  
#write.csv(quanteda_test, "quanteda_test.csv")

```

### 
