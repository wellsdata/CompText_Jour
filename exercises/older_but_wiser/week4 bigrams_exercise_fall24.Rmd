---
title: "Lynching Text Analysis"
author: "Rob Wells"
date: '2022-09-27'
output: html_document
---

# Jour 389/689 Fall 2024:

```{r}
#load tidyverse, tidytext, rio and quanteda libraries

library(tidyverse)
library(tidytext)


```

```{r}
#Import dataframe 


lynch <- read_csv("../data/articles_oct_19.csv")

```

# plot of years covered

```{r}

#Show range of years covered
years_ct <- lynch %>%
  distinct(filename, .keep_all = TRUE) %>% 
  count(year)

y <- lynch %>%
  distinct(filename, .keep_all = TRUE)

#Create chart of years
ggplot(years_ct,aes(x = year, y = n,
             fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  labs(title = "Years of Lynching Coverage",
       subtitle = "Based in 7,162 extracted articles",
       caption = "Graphic by Rob Wells, 10-30-2023",
       y="Articles",
       x="Year")

# ggsave("../output_images_tables/Figure2_years_lynching_coverage_10.30.23.png",device = "png",width=9,height=6, dpi=800)
```

# By decade

## post1940

```{r}
#Filter articles from 1940s forward
post1940 <-  lynch %>% 
  filter(year >= 1940)

post1940 %>% 
  select(filename) %>% 
 distinct(filename, .keep_all = TRUE) %>% 
  count(filename) %>% 
  summarize(total =sum(n)) 
#62 articles 

statespost1940 <- post1940 %>% 
  select(newspaper_state, filename) %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  count(newspaper_state) %>% 
  arrange(desc(n))

statespost1940 %>% 
  select(newspaper_state, n) %>% 
slice_max(n, n=10)
#newspaper_state

# Michigan	20			
# Minnesota	18			
# District of Columbia	5			
# Nebraska	4			
# Illinois	3			
# Mississippi	2			
# North Carolina	2			
# Washington	2			
# Alaska	1			
# Arizona	1	

#Fact Check
#sum(statesthe1850s$n)

x <- post1940 %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  arrange(date)

#write_csv(x, "post1940_index.csv")
```

# Tokenize

```{r}

stories <- str_replace_all(post1940$sentence, "- ", "")
stories_df <- tibble(stories,)

# unnest includes lower, punct removal

stories_tokenized <- stories_df %>%
  unnest_tokens(word,stories)

stories_tokenized
```


#Remove stopwords
The tidytext package includes the stop_words dataset.It contains, as of this writing, 1,149 words that data scientists and linguistic nerds felt could be removed from sentences because they don't add meaning. Filtering out these words can help focus on the more meaningful content, making it easier to uncover trends, themes, and key information in large amounts of text. Obviously, we have different priorities and we may or may not want to use stop_words or we have want to provide a customized list of stop words.

The stop_words list is derived from three separate lists, or lexicons: SMART (571 words), onix (404 words), and snowball (174 words)

The ONIX lexicon comes from the Open Information Exchange and is often used in text mining and natural language processing. 

The Snowball lexicon is part of a broader project that has algorithms that simplify words in different languages by reducing them to their root form. It's best known for the Porter stemming algorithm, which, for example, changes "running" to "run." 

Lastly, the SMART lexicon is a set of common words, like "and," "the," and "is," and it comes from the SMART Information Retrieval System, created at Cornell University in the 1960s.

```{r}
data(stop_words)

test <- stop_words %>% 
  as.data.frame()

head(test)
```

#Remove stopwords
```{r}

data(stop_words)

stories_tokenized <- stories_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))

# Word Count

story_word_ct <- stories_tokenized %>%
  count(word, sort=TRUE)

#write_csv(lynch_word_ct, "lynching_corpus_word_count.csv")

```

# Bigrams

```{r}
bigrams <- stories_df %>%
  unnest_tokens(bigram, stories, token="ngrams", n=2)

bigrams

#Filter out stop words.


bigrams1 <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams2 <- bigrams1 %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram3 <- bigrams2 %>%
  count(word1, word2, sort = TRUE)

#replace Date for the decade analyzed
bigram3 <- bigram3 %>% 
  mutate(decade = "post1940")

#write_csv(stories_bigram_cts_post1940, "../output/post1940_lynch_bigram_count.csv")

```

# Trigrams

```{r}
trigrams <- stories_df %>%
  unnest_tokens(trigram, stories, token="ngrams", n=3)

trigrams2 <- trigrams %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

trigrams3 <- trigrams2 %>%
  count(word1, word2, word3, sort = TRUE)

#filtered
# stories_trigrams_filtered <- stories_trigrams_separated %>%
#   filter(!word1 %in% stop_words$word) %>%
#   filter(!word2 %in% stop_words$word) %>%
#   filter(!word3 %in% stop_words$word)
# 
# stories_trigrams_ct <- stories_trigrams_filtered %>%
#   count(word1, word2, word3, sort = TRUE)

#replace Date for the decade analyzed
stories_trigrams_ct_post1940 <- stories_trigrams_ct %>% 
  mutate(decade = "post1940")

#write_csv(stories_trigrams_ct_post1940, "../output/post1940_lynch_trigram_count.csv")


```


#Notes Below#

#-------------------------------------------------------------------------------


#Compile DFs

```{r}
stories_bigram_cts_pre1850s <- read.csv("../output/pre1850s_lynch_bigram_count.csv")
stories_bigram_cts_the1850s <- read.csv("../output/the1850s_lynch_bigram_count.csv")
stories_bigram_cts_the1860s <- read.csv("../output/1860s_lynch_bigram_count.csv")
stories_bigram_cts_the1870s <- read.csv("../output/1870s_lynch_bigram_count.csv")
stories_bigram_cts_the1880s <- read.csv("../output/1880s_lynch_bigram_count.csv")
stories_bigram_cts_the1890s <- read.csv("../output/1890s_lynch_bigram_count.csv")
stories_bigram_cts_the1900s <- read.csv("../output/1900s_lynch_bigram_count.csv")
stories_bigram_cts_the1910  <- read.csv("../output/1910s_lynch_bigram_count.csv")
stories_bigram_cts_the1920s <- read.csv("../output/1920s_lynch_bigram_count.csv")
```

```{r}
#Compile DFs

bigrams_all <- rbind(stories_bigram_cts_pre1850s,stories_bigram_cts_the1850s, stories_bigram_cts_the1860s, stories_bigram_cts_the1870s, stories_bigram_cts_the1880s, stories_bigram_cts_the1890s, stories_bigram_cts_the1900s, stories_bigram_cts_the1910,stories_bigram_cts_the1920s, stories_bigram_cts_1930s, stories_bigram_cts_post1940) 


write.csv(bigrams_all, "../output/all_bigrams_11.10.csv")
```

```{r}
stories_QUINTgrams <- stories_df %>%
  unnest_tokens(phrase, stories, token="ngrams", n=5)

stories_QUINTgrams_ct <- stories_QUINTgrams %>%
  count(phrase, sort=TRUE)

#write_csv(stories_QUINTgrams_ct, "stories_corpus_quintgram_count.csv")

stories_QUINTgrams_ct

```

```{r}
# plotting for fun and profit
#NEEDS TO BE FIXED
story_word_ct %>%
  filter(n >= 5000) %>%
  mutate(word = reorder(word,n)) %>%
  ggplot(aes(n, word)) + 
  geom_col() +
  labs(y = NULL) + 
  ggtitle("Words mentioned 5000 times or more in identified lynching articles")

```

*If we want to look at trigrams for narrative, maybe we don't drop the stop words? Might be more likely to catch turns of phrase.*

# Quanteda

```{r}
#install.packages("readtext")
library(quanteda)
library(readtext)
lynch1 <- readtext("../narratives/article_text_test")
```

### creates index with metadata

```{r}
###
# List out text files that match pattern .txt, create DF
###

files <- list.files("../article_text_7_15/article_text/", pattern="*.txt") %>% 
  as.data.frame() %>%
  rename(filename = 1) %>%
  filter(!str_detect(filename,"log"))


###
# Load 638 stories provided by jack, create join column, join to files list
###

jackindex <- read_csv("../article_text_7_15/article_text/LayoutBoxes_index.csv") %>%
  mutate(filename = paste0(file_id,"_",article_id,".txt")) %>%
  inner_join(files) %>%
  mutate(filepath = paste0("../article_text_7_15/article_text/",filename))
```

### adds metadata to corpus

```{r}
lynch1 <- lynch1 %>% 
  inner_join(jackindex, by=c("doc_id"="filename"))


#Other options
#summary(corpus_subset(data_corpus_inaugural, President == "Adams"))

```

```{r}

my_corpus <- corpus(lynch1)  # build a new corpus from the texts
summary(my_corpus)


```

### subset corpus

```{r}

x1920s <- summary(corpus_subset(my_corpus, year > 1920))

```

## kwic

```{r}

quanteda_test <- kwic(my_corpus, "lynch", valuetype = "regex") %>% as.data.frame()


quanteda_test <- kwic(my_corpus, "torture", valuetype = "regex") %>% as.data.frame()

quanteda_test <- kwic(my_corpus, "watts", valuetype = "regex") %>% as.data.frame()

  
#write.csv(quanteda_test, "quanteda_test.csv")

```

### 
