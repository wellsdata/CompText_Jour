  ---
title: Japanese
weight: 40
draft: false
---

{{% author %}}By Kohei Watanabe{{% /author %}} 

```{r, message=FALSE}
require(quanteda)
require(quanteda.textstats)
require(quanteda.corpora)
options(width = 110)
```

We remove grammatical words using `stopwords("ja", source = "marimo")`. You can select tokens only with Japanese words (Hiragana, Katakana, Kanji) with `"^[ぁ-んァ-ヶー一-龠]+$"`. There are also Unicode character classes for hiragana (`\p{script=Hira}`) and katakana (`\p{script=Kana}`) that you can use.

```{r}
# reshape document to the level of paragraphs
corp <- corpus_reshape(data_corpus_udhr["jpn"], to = "paragraphs")

# tokenize corpus and apply pre-processing
toks <- tokens(corp, remove_punct = TRUE, remove_numbers = TRUE, padding = TRUE) %>% 
  tokens_remove(pattern = stopwords("ja", source = "marimo"), padding = TRUE) %>% 
  tokens_select(pattern = "^[ぁ-んァ-ヶー一-龠]+$", valuetype = "regex", padding = TRUE)
print(toks[2], max_ndoc = 1, max_ntok = -1)
```

We can improve tokenization by collocation analysis in a similar way as [compounding multi-word expressions](advanced-operations/compound-mutiword-expressions/) in English texts. We identify collocations of katakana or kanji (`"^[ァ-ヶー一-龠]+$"`) using `textstat_collocations()`. We set `padding = TRUE` to keep the distance between words.

```{r}
# perform collocation analysis
tstat_col <- toks %>% 
  tokens_select("^[ァ-ヶー一-龠]+$", valuetype = "regex", padding = TRUE) %>%  
  textstat_collocations()
head(tstat_col, 10)
```

After compounding of statistically significantly associated collocations (`tstat_col$z > 3`), we can resort to the lengths of words (`min_nchar = 2`) to further remove grammatical words.

```{r}
# compound collocations
toks_comp <- tokens_compound(toks, tstat_col[tstat_col$z > 3,], concatenator = "") %>% 
  tokens_keep(min_nchar = 2)
print(toks_comp[2], max_ndoc = 1, max_ntok = -1)
```

```{r}
# construct document-feature matrix
dfmat <- dfm(toks_comp)
print(dfmat)
```
