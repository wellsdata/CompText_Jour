---
title: English and German
weight: 10
draft: false
---

{{% author %}}By Kohei Watanabe and Stefan Müller{{% /author %}} 

```{r, message=FALSE}
require(quanteda)
require(quanteda.corpora)
options(width = 110)
```

## English 

After tokenization, we remove so called "stopwords" using `stopwords("en", source = "marimo")`. If you want tokens to comprise only of the English alphabet, you can select them by `"^[a-zA-Z]+$"`. You can find more details on stopwords on the [website](http://stopwords.quanteda.io) of the **stopwords** package.  Please be very careful when pre-processing or removing tokens since these choices [might influence subsequent results](https://doi.org/10.1017/pan.2017.44).

```{r}
# reshape corpus to the level of paragraphs
corp_eng <- corpus_reshape(data_corpus_udhr["eng"], to = "paragraphs")

# tokenize corpus and apply pre-processing
toks_eng <- tokens(corp_eng, remove_punct = TRUE, remove_numbers = TRUE) %>% 
  tokens_remove(pattern = stopwords("en", source = "marimo")) %>% 
  tokens_keep(pattern = "^[a-zA-Z]+$", valuetype = "regex")
print(toks_eng[2], max_ndoc = 1, max_ntoken = -1)
```

```{r}
# construct a document-feature matrix
dfmat_eng <- dfm(toks_eng)
print(dfmat_eng)
```

## German

Pre-processing of German texts is very similar to English texts, but we have to use [Unicode character class](http://www.unicode.org/reports/tr31/#Table_Recommended_Scripts) `"^[\\p{script=Latn}]+$"` to include characters with umlauts (ä/ö/ü).

```{r}
# reshape document to the level of paragraphs
corp_ger <- corpus_reshape(data_corpus_udhr["deu_1996"], to = "paragraphs")

# tokenize corpus and apply pre-processing
toks_ger <- tokens(corp_ger, remove_punct = TRUE, remove_numbers = TRUE) %>% 
  tokens_remove(pattern = stopwords("de", source = "marimo")) %>% 
  tokens_keep(pattern = "^[\\p{script=Latn}]+$", valuetype = "regex")
print(toks_ger[2], max_ndoc = 1, max_ntoken = -1)
```

```{r}
# construct document-feature matrix
dfmat_ger <- dfm(toks_ger)
print(dfmat_ger)
```
