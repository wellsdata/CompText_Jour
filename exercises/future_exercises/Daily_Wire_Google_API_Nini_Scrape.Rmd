---
title: "Daily Wire Scraper"
author: "Wells"
date: "04-15-2025"
  output:
  html_document: default
  pdf_document: default
---

```{r}
# Install and load required packages if not already installed
#install.packages("rvest")
library(rvest)
library(tidyverse)
```

```{r}
# Install and load required packages


# Set up API credentials
# You'll need to get these from Google Cloud Console
# Visit: https://console.cloud.google.com/
# 1. Create a project
# 2. Enable Custom Search API
# 3. Create credentials
# 4. Set up a Custom Search Engine at https://programmablesearchengine.google.com/

install.packages("googleAuthR")
library(googleAuthR)
# Set your API key and Search Engine ID
#set_google_api_key("XXX")
#set_google_custom_search_engine_id("XXX")

```

```{r}
# Install and load required packages
# install.packages("httr")
# install.packages("jsonlite")
library(httr)
library(jsonlite)
```

```{r}
# Function to search using Google Custom Search API with pagination
google_search_all <- function(api_key, search_engine_id) {
  base_url <- "https://www.googleapis.com/customsearch/v1"
  all_results <- data.frame(
    url = character(),
    title = character(),
    snippet = character(),
    stringsAsFactors = FALSE
  )
  
  # The exact search query
  full_query <- 'site:www.dailywire.com "kamala harris" after:2024-07-21 before:2024-11-05'
  
  # Google only allows up to 100 pages (1000 results)
  for(start_index in seq(1, 300, 10)) {
    # Add delay to avoid hitting rate limits
    Sys.sleep(1)
    
    response <- GET(
      base_url,
      query = list(
        key = api_key,
        cx = search_engine_id,
        q = full_query,
        start = start_index
      )
    )
    
    results <- fromJSON(rawToChar(response$content))
    
    # Check if we have items in the response
    if(!is.null(results$items)) {
      page_df <- data.frame(
        url = results$items$link,
        title = results$items$title,
        snippet = results$items$snippet,
        stringsAsFactors = FALSE
      )
      
      all_results <- rbind(all_results, page_df)
      
      # Print progress
      cat("Retrieved results", start_index, "to", start_index + nrow(page_df) - 1, "\n")
    } else {
      # If no more results, break the loop
      break
    }
  }
  
  return(all_results)
}

# Your credentials here
api_key <- "XXXX"
search_engine_id <- "XXXX"
# Get all results
all_urls_df <- google_search_all(api_key, search_engine_id)

# Remove any duplicates
sept_urls_df <- unique(all_urls_df)

# Print total number of results
cat("\nTotal unique articles found:", nrow(all_urls_df), "\n")

# Save to CSV
write.csv(sept_urls_df, "sept_kamala_harris_all_articles.csv", row.names = FALSE)
```
#determine search results per month

```{r}
library(httr)
library(jsonlite)
library(dplyr)
library(lubridate)

# Function to count search results by date
count_results_by_date <- function(api_key, search_engine_id, site, search_term, date_ranges) {
  results_count <- data.frame(
    date_range = character(),
    result_count = numeric(),
    stringsAsFactors = FALSE
  )
  
  for (date_range in date_ranges) {
    # Create the full query
    query <- paste0('site:', site, ' "', search_term, '" after:', date_range[1], ' before:', date_range[2])
    
    # Make the API request
    response <- GET(
      "https://www.googleapis.com/customsearch/v1",
      query = list(
        key = api_key,
        cx = search_engine_id,
        q = query,
        num = 1  # We only need the count, not actual results
      )
    )
    
    # Parse the response
    if (status_code(response) == 200) {
      results <- fromJSON(rawToChar(response$content))
      
      # Get the total count
      total_results <- as.numeric(results$searchInformation$totalResults)
      
      # Add to dataframe
      new_row <- data.frame(
        date_range = paste(date_range[1], "to", date_range[2]),
        result_count = total_results,
        stringsAsFactors = FALSE
      )
      
      results_count <- rbind(results_count, new_row)
      
      message("Query: ", query)
      message("Results: ", total_results)
    } else {
      message("Error for date range ", date_range[1], " to ", date_range[2], 
              ": Status code ", status_code(response))
    }
    
    # Delay to avoid hitting rate limits
    Sys.sleep(1)
  }
  
  return(results_count)
}

# Define your parameters
api_key <- "XXXX"
search_engine_id <- "XXXX"
site <- "www.dailywire.com"
search_term <- "kamala harris"

# Define date ranges to check (monthly intervals)
date_ranges <- list(
  c("2024-07-01", "2024-07-31"),
  c("2024-08-01", "2024-08-31"),
  c("2024-09-01", "2024-09-30"),
  c("2024-10-01", "2024-10-31"),
  c("2024-11-01", "2024-11-05")
)

# If you want to see how results are distributed within September
# september_ranges <- list(
#   c("2024-09-01", "2024-09-10"),
#   c("2024-09-11", "2024-09-20"),
#   c("2024-09-21", "2024-09-30")
# )

# If you want to see how results are distributed within a month

august_ranges <- list(
  c("2024-08-01", "2024-08-15"),
  c("2024-08-16", "2024-08-32")
)

october_ranges <- list(
  c("2024-10-01", "2024-10-10"),
  c("2024-10-11", "2024-10-20"),
  c("2024-10-21", "2024-10-31")
)

# Run the count for monthly data
monthly_counts <- count_results_by_date(api_key, search_engine_id, site, search_term, date_ranges)

# Run the count for September segments
october_counts <- count_results_by_date(api_key, search_engine_id, site, search_term, october_ranges)
august_counts <- count_results_by_date(api_key, search_engine_id, site, search_term, august_ranges)
# View results
print(monthly_counts)
print(october_counts)
print(august_counts)
# # Create a simple bar chart of monthly results
# barplot(monthly_counts$result_count, 
#         names.arg = monthly_counts$date_range,
#         main = "DailyWire Articles Mentioning 'Kamala Harris' by Month",
#         xlab = "Month", 
#         ylab = "Number of Articles",
#         col = "steelblue")
```



#delivers 100 results

```{r}
# Function to search using Google Custom Search API
google_search <- function(api_key, search_engine_id, start = 1) {
  base_url <- "https://www.googleapis.com/customsearch/v1"
  
  # The exact search query we want
  full_query <- 'site:www.dailywire.com "kamala harris" after:2024-07-21 before:2024-11-05'
  
  response <- GET(
    base_url,
    query = list(
      key = api_key,
      cx = search_engine_id,
      q = full_query,
      start = start
    )
  )
  
  results <- fromJSON(rawToChar(response$content))
  return(results)
}

# Get results
results <- google_search(api_key, search_engine_id)

# Create dataframe with URL, title, and snippet
urls_df <- data.frame(
  url = results$items$link,
  title = results$items$title,
  snippet = results$items$snippet,
  stringsAsFactors = FALSE
)

# Print the results
print(urls_df)

# Save to CSV
#write.csv(urls_df, "kamala_harris_articles.csv", row.names = FALSE)
```

older version

```{r}

# Function to search using Google Custom Search API
google_search <- function(query, api_key, search_engine_id, start = 1) {
  base_url <- "https://www.googleapis.com/customsearch/v1"
  
  # Construct the URL with parameters
  response <- GET(
    base_url,
    query = list(
      key = api_key,
      cx = search_engine_id,
      q = query,
      start = start
    )
  )
  
  # Parse the response
  results <- fromJSON(rawToChar(response$content))
  
  return(results)
}

# Your credentials here
api_key <- "XXX"
search_engine_id <- "XXXX"

# Your search query
query <- 'site:www.dailywire.com before:2024-07-21 and after:2024-11-05'

# Get results
results <- google_search(query, api_key, search_engine_id)

# Create dataframe from the results
urls_df <- data.frame(
  url = results$items$link,
  title = results$items$title,
  stringsAsFactors = FALSE
)

# View results
# print(urls_df)
# 
# # Save to CSV
# write.csv(urls_df, "dailywire_urls.csv", row.names = FALSE)
```

Custom search engine script

```{=html}
<script async src="https://cse.google.com/cse.js?cx=d105d1b8f6b3543ba">
</script>
```
::: gcse-search
:::

https://www.google.com/search?q=site:www.dailywire.com+%22kamala+harris%22+after:2024-07-21+before:2024-11-05&sca_esv=2727c2a8608e1f45&sxsrf=AHTn8zpDXqi-tyT-WzRBtigEJlgI_ryXoQ:1739305367342&ei=l7GrZ7bHFIuh5NoP0PXu0Qk&start=10&sa=N&sstk=Af40H4WUIxjStGfGhskfy2OrTCAmktGQTUGNIcqW3wBTgDRPrqc67yhnWy6g2QaNNH4AdXcRVqUiI527k8ngwSv3eP1EHSzhSy4ciQ&ved=2ahUKEwj27s20ubyLAxWLEFkFHdC6O5oQ8tMDegQIBxAE&biw=1920&bih=934&dpr=2

# Build a table from the Daily Wire RSS feed

```{r}
# First install and load required packages
install.packages(c("xml2", "tidyRSS"))
library(xml2)
library(tidyRSS)

# Assuming you have the RSS content stored in a variable called 'rss_content'
# You would need to use your preferred method to get the RSS content first

# Parse the RSS feed
df <- tidyfeed("https://www.dailywire.com/feeds/rss.xml") 

# This will create a dataframe with common RSS fields like:
# - feed_title
# - feed_link
# - item_title
# - item_link
# - item_description
# - item_pub_date

# You can then manipulate the dataframe as needed
# For example, to see the first few rows:
head(df)

# To select specific columns:
selected_df <- df[, c("item_title", "item_pub_date", "item_link")]
```

# Scrape Wiki page for racist terms

## Thanks to Sean Mussenden for his Advanced rvest tutorial, which this is shamelessly and ruthlessly stolen and remixed

https://github.com/smussenden/datajournalismbook

```{r}
# install.packages("tidyverse")
# install.packages("rvest")
# install.packages("janitor")

library(tidyverse)
library(rvest)
library(janitor)
library(readxl)
```

Example URL of pages I want to scrape https://www.dailywire.com/news/not-encouraging-nyt-warns-kamala-in-trouble-as-poll-finds-her-tied-with-trump?topStoryPosition=1

#I asked ChatGPT: build a webscraper this into a function so it will extract URLs from Daily_Wire_Articles.xlsx from the URL column and produce one text file per article. there are 20 URLs on that list.

```{r}


# Define the function to scrape a Daily Wire article
scrape_article <- function(url) {
  # Read the HTML content of the page
  page <- tryCatch(
    read_html(url),
    error = function(e) {
      message(paste("Error reading:", url))
      return(NULL)
    }
  )
  
  # Return NULL if page could not be loaded
  if (is.null(page)) return(NULL)

  # Extract the headline (assuming it's in an <h1> tag)
  headline <- page %>%
    html_element("h1") %>%
    html_text(trim = TRUE)

  # Extract the article text (all <p> elements)
  article_text <- page %>%
    html_elements("p") %>%
    html_text(trim = TRUE) %>%
    paste(collapse = "\n")

  # Combine headline and article into one string
  full_text <- paste("Headline:\n", headline, "\n\nArticle Text:\n", article_text)
  
  return(full_text)
}

# Read the URLs from the Excel file
# You will need to point to the location of your Daily Wire Articles spreadsheet.
urls <- read_excel("~/Code/CompText_Jour/data/Daily_Wire_Articles.xlsx", sheet = 1)$URL

# Loop over URLs and save each article as a text file
for (url in urls) {
  article_content <- scrape_article(url)
  
  # Skip if article content is NULL
  if (is.null(article_content)) next

  # Create a filename from the URL (using only the last part)
  filename <- paste0(basename(url), ".txt")
  
  # Save the article content to a text file
  writeLines(article_content, con = filename)
  
  message(paste("Saved:", filename))
}

```

#Basic scraper for one article

```{r}
library(rvest)

# Define the URL
url2 <- "https://www.dailywire.com/news/not-encouraging-nyt-warns-kamala-in-trouble-as-poll-finds-her-tied-with-trump?topStoryPosition=1"

# Read the HTML content of the page
page <- read_html(url2)

# Extract the headline (usually in an <h1> tag)
headline <- page %>%
  html_element("h1") %>%
  html_text(trim = TRUE)

# Extract the article text (all <p> elements)
article_text <- page %>%
  html_elements("p") %>%
  html_text(trim = TRUE)

# Print the headline and article content
cat("Headline:\n", headline, "\n\n")
cat("Article Text:\n", paste(article_text, collapse = "\n"))

```

#ChatGPT explanation:

html_element("h1"): Extracts the headline, assuming it is in an

<h1>

tag. If the headline isn't inside

<h1>

, you might need to inspect the webpage's HTML to adjust the selector. trim = TRUE: Ensures that any extra spaces or newlines around the text are removed. paste(..., collapse = "\n"): Joins the article paragraphs into a readable format with line breaks.
